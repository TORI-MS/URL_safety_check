import streamlit as st
import pandas as pd
import joblib
import numpy as np
import re
from urllib.parse import urlparse
import matplotlib.pyplot as plt
import seaborn as sns

# ---------------------------
# ëª¨ë¸ & ë°ì´í„° ë¡œë“œ
# ---------------------------
model = joblib.load("phishing_model.joblib")   # í˜„ì¬ í™˜ê²½ì—ì„œ í•™ìŠµí•œ ëª¨ë¸
known_urls = pd.read_csv("famous_url_with_alias.csv")  # ì¸ì¦ëœ URL ëª©ë¡
dataset = pd.read_csv("dataset_phishing.csv")  # ë¶„ì„ìš© ë°ì´í„°ì…‹
X = dataset.drop(columns=["url", "status"])
feature_columns = X.columns.tolist()  # feature ìˆœì„œ

# ---------------------------
# Feature Extraction Function
# ---------------------------
def extract_features(url: str):
    parsed = urlparse(url)
    hostname = parsed.netloc or ""
    path = parsed.path or ""

    values = {}

    # ì£¼ìš” feature ê³„ì‚°
    values["length_url"] = len(url)
    values["length_hostname"] = len(hostname)
    values["ip"] = 1 if re.match(r"\d+\.\d+\.\d+\.\d+", hostname) else 0
    values["nb_dots"] = url.count(".")
    values["nb_hyphens"] = url.count("-")
    values["nb_at"] = url.count("@")
    values["nb_qm"] = url.count("?")
    values["nb_and"] = url.count("&")
    values["nb_or"] = url.count("|")
    values["nb_eq"] = url.count("=")
    values["nb_underscore"] = url.count("_")
    values["nb_tilde"] = url.count("~")
    values["nb_percent"] = url.count("%")
    values["nb_slash"] = url.count("/")
    values["nb_star"] = url.count("*")
    values["nb_colon"] = url.count(":")
    values["nb_comma"] = url.count(",")
    values["nb_semicolumn"] = url.count(";")
    values["nb_dollar"] = url.count("$")
    values["nb_space"] = url.count(" ")
    values["nb_www"] = url.count("www")
    values["nb_com"] = url.count(".com")
    values["nb_dslash"] = url.count("//")
    values["http_in_path"] = 1 if "http" in path else 0
    values["https_token"] = 1 if "https" in url else 0
    values["ratio_digits_url"] = sum(c.isdigit() for c in url) / len(url) if len(url) > 0 else 0
    values["ratio_digits_host"] = sum(c.isdigit() for c in hostname) / len(hostname) if len(hostname) > 0 else 0
    values["punycode"] = 1 if "xn--" in hostname else 0
    values["port"] = 0
    values["tld_in_path"] = 1 if re.search(r"\.[a-z]{2,}$", path) else 0
    values["tld_in_subdomain"] = 1 if re.search(r"\.[a-z]{2,}$", hostname.split(".")[0]) else 0
    values["abnormal_subdomain"] = 1 if hostname.count(".") > 3 else 0
    values["nb_subdomains"] = len(hostname.split(".")) - 2 if hostname else 0
    values["prefix_suffix"] = 1 if "-" in hostname else 0
    values["random_domain"] = 0

    # ë‚˜ë¨¸ì§€ featureëŠ” ê¸°ë³¸ê°’(0)
    for col in feature_columns:
        if col not in values:
            values[col] = 0

    features = [values[col] for col in feature_columns]
    return np.array(features, dtype=float)

# ---------------------------
# í•œêµ­ì–´ ì¹œí™”í˜• Feature ì´ë¦„ ë§¤í•‘
# ---------------------------
feature_names_kr = {
    # ê¸¸ì´ ê´€ë ¨
    "length_url": "URL ì „ì²´ ê¸¸ì´",
    "length_hostname": "ë„ë©”ì¸ ê¸¸ì´",

    # íŠ¹ìˆ˜ë¬¸ì ê°œìˆ˜
    "nb_dots": "ì (.) ê°œìˆ˜",
    "nb_hyphens": "í•˜ì´í”ˆ(-) ê°œìˆ˜",
    "nb_at": "@ ê¸°í˜¸ ê°œìˆ˜",
    "nb_qm": "ë¬¼ìŒí‘œ(?) ê°œìˆ˜",
    "nb_and": "& ê¸°í˜¸ ê°œìˆ˜",
    "nb_or": "| ê¸°í˜¸ ê°œìˆ˜",
    "nb_eq": "= ê¸°í˜¸ ê°œìˆ˜",
    "nb_underscore": "ë°‘ì¤„(_) ê°œìˆ˜",
    "nb_tilde": "ë¬¼ê²°(~) ê°œìˆ˜",
    "nb_percent": "% ê¸°í˜¸ ê°œìˆ˜",
    "nb_slash": "ìŠ¬ë˜ì‹œ(/) ê°œìˆ˜",
    "nb_star": "* ê¸°í˜¸ ê°œìˆ˜",
    "nb_colon": "ì½œë¡ (:) ê°œìˆ˜",
    "nb_comma": "ì‰¼í‘œ(,) ê°œìˆ˜",
    "nb_semicolumn": "ì„¸ë¯¸ì½œë¡ (;) ê°œìˆ˜",
    "nb_dollar": "$ ê¸°í˜¸ ê°œìˆ˜",
    "nb_space": "ê³µë°±( ) ê°œìˆ˜",
    "nb_www": "www ë¬¸ìì—´ ê°œìˆ˜",
    "nb_com": ".com ë¬¸ìì—´ ê°œìˆ˜",
    "nb_dslash": "// ë¬¸ìì—´ ê°œìˆ˜",

    # ë³´ì•ˆ ê´€ë ¨
    "https_token": "HTTPS í¬í•¨ ì—¬ë¶€",
    "http_in_path": "ê²½ë¡œì— http í¬í•¨ ì—¬ë¶€",

    # ìˆ«ì ë¹„ìœ¨
    "ratio_digits_url": "ìˆ«ì ë¹„ìœ¨(ì „ì²´ URL)",
    "ratio_digits_host": "ìˆ«ì ë¹„ìœ¨(ë„ë©”ì¸)",

    # ë„ë©”ì¸ ê´€ë ¨
    "punycode": "êµ­ì œë„ë©”ì¸(Punycode) ì—¬ë¶€",
    "nb_subdomains": "ì„œë¸Œë„ë©”ì¸ ê°œìˆ˜",
    "prefix_suffix": "ë„ë©”ì¸ì— í•˜ì´í”ˆ(-) í¬í•¨ ì—¬ë¶€",
    "abnormal_subdomain": "ë¹„ì •ìƒì  ì„œë¸Œë„ë©”ì¸ ì—¬ë¶€",
    "random_domain": "ëœë¤ ë„ë©”ì¸ ì—¬ë¶€(ì¶”ì •)",

    # ì¶”ê°€ë¡œ í•„ìš”í•˜ë©´ ê³„ì† í™•ì¥ ê°€ëŠ¥
}

# ---------------------------
# Streamlit UI
# ---------------------------
st.title("ğŸ” í”¼ì‹± URL íƒì§€ê¸°")

user_url = st.text_input("ê²€ì‚¬í•  URLì„ ì…ë ¥í•˜ì„¸ìš”:")

if user_url:
    # 1. ì¸ì¦ëœ URL ì²´í¬
    if user_url in known_urls["url"].values:
        st.success("âœ… ì¸ì¦ëœ ì•ˆì „í•œ URLì…ë‹ˆë‹¤.")
    else:
        # 2. íŠ¹ì§• ì¶”ì¶œ & ì˜ˆì¸¡
        features = extract_features(user_url)
        if features.shape[0] != model.n_features_in_:
            st.error(f"âŒ Feature ê°œìˆ˜ ë¶ˆì¼ì¹˜: {features.shape[0]}ê°œ vs ëª¨ë¸ {model.n_features_in_}ê°œ")
        else:
            pred = model.predict([features])[0]

            if pred == "phishing":
                st.error("ğŸš¨ í”¼ì‹± ìœ„í—˜ URLë¡œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤!")
            else:
                st.success("âœ… ì•ˆì „í•œ URLë¡œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤.")

            # ---------------------------
            # ğŸ“Š ë¶„ì„ ë° ì„¤ëª…
            # ---------------------------
            st.subheader("ğŸ“Š ë¶„ì„ ê·¼ê±°")

            # (1) Feature ì¤‘ìš”ë„ (ì „ì²´ ëª¨ë¸ ê¸°ì¤€)
            st.markdown("**ëª¨ë¸ì´ í•™ìŠµí•œ ì£¼ìš” íŠ¹ì§• ì¤‘ìš”ë„**")
            importances = model.feature_importances_
            indices = np.argsort(importances)[-15:]
            plt.figure(figsize=(6,5))
            sns.barplot(x=importances[indices], y=np.array(feature_columns)[indices])
            plt.title("ì£¼ìš” Feature ì¤‘ìš”ë„")
            st.pyplot(plt)

            # (2) ì…ë ¥ URL ê°’ ë¹„êµ
            st.markdown("**ì…ë ¥í•œ URLì´ ì •ìƒ/í”¼ì‹± í‰ê· ê³¼ ë¹„êµí–ˆì„ ë•Œ ì–´ë–¤ íŠ¹ì„±ì„ ë³´ì´ëŠ”ì§€**")
            legit_mean = dataset[dataset["status"]=="legitimate"].drop(columns=["url","status"]).mean()
            phish_mean = dataset[dataset["status"]=="phishing"].drop(columns=["url","status"]).mean()

            explanation = []
            for i, col in enumerate(feature_columns[:30]):  # ì²˜ìŒ 30ê°œë§Œ ë¹„êµ
                val = features[i]
                col_name = feature_names_kr.get(col, col)  # í•œêµ­ì–´ ì´ë¦„ ìˆìœ¼ë©´ ë³€í™˜
                if abs(val - phish_mean[col]) < abs(val - legit_mean[col]):
                    explanation.append(f"ğŸ”´ {col_name} ê°’({val:.2f}) â†’ í”¼ì‹± ì‚¬ì´íŠ¸ í‰ê· ({phish_mean[col]:.2f})ì— ë” ê°€ê¹Œì›€ â†’ í”¼ì‹± ì˜ì‹¬")
                else:
                    explanation.append(f"ğŸŸ¢ {col_name} ê°’({val:.2f}) â†’ ì •ìƒ ì‚¬ì´íŠ¸ í‰ê· ({legit_mean[col]:.2f})ì— ë” ê°€ê¹Œì›€ â†’ ì •ìƒì  íŠ¹ì„±")

            for exp in explanation[:10]:  # ìƒìœ„ 10ê°œë§Œ ì¶œë ¥
                st.write(exp)
